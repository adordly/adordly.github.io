<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>深度学习语义分割理论与实战指南 | 咳咳</title><meta name="keywords" content="人工智能"><meta name="author" content="咳咳"><meta name="copyright" content="咳咳"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="深度学习语义分割理论与实战指南A Theory and Practical Guide to Deep Learning Semantic Segmentation     v1.0 louwillMachine Learning Lab                  Fig0. Machine Learning Lab             引言图像分类、目标检测和图像分割是基于深度学习的"><meta property="og:type" content="article"><meta property="og:title" content="深度学习语义分割理论与实战指南"><meta property="og:url" content="https://adordly.github.io/2022/02/25/deepLearningSemanticSegmentation/index.html"><meta property="og:site_name" content="咳咳"><meta property="og:description" content="深度学习语义分割理论与实战指南A Theory and Practical Guide to Deep Learning Semantic Segmentation     v1.0 louwillMachine Learning Lab                  Fig0. Machine Learning Lab             引言图像分类、目标检测和图像分割是基于深度学习的"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://adordly.github.io/img/text/deeplearning.png"><meta property="article:published_time" content="2022-02-25T03:46:47.000Z"><meta property="article:modified_time" content="2022-02-26T14:59:09.665Z"><meta property="article:author" content="咳咳"><meta property="article:tag" content="人工智能"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://adordly.github.io/img/text/deeplearning.png"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="https://adordly.github.io/2022/02/25/deepLearningSemanticSegmentation/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js",css:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"深度学习语义分割理论与实战指南",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-02-26 22:59:09"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0!==o){const a=new Date;o=864e5*o,t={value:t,expiry:a.getTime()+o};localStorage.setItem(e,JSON.stringify(t))}},get:function(e){var t=localStorage.getItem(e);if(t){t=JSON.parse(t);const o=new Date;if(!(o.getTime()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=a=>new Promise((t,e)=>{const o=document.createElement("script");o.src=a,o.async=!0,o.onerror=e,o.onload=o.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(o.onload=o.onreadystatechange=null,t())},document.head.appendChild(o)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme"),"dark"===e?activateDarkMode():"light"===e&&activateLightMode(),e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/social_img.jpeg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/text/deeplearning.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">咳咳</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习语义分割理论与实战指南</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-02-25T03:46:47.000Z" title="发表于 2022-02-25 11:46:47">2022-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-02-26T14:59:09.665Z" title="更新于 2022-02-26 22:59:09">2022-02-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>45分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="深度学习语义分割理论与实战指南"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="深度学习语义分割理论与实战指南"><a href="#深度学习语义分割理论与实战指南" class="headerlink" title="深度学习语义分割理论与实战指南"></a><strong>深度学习语义分割理论与实战指南</strong></h1><h1 id="A-Theory-and-Practical-Guide-to-Deep-Learning-Semantic-Segmentation"><a href="#A-Theory-and-Practical-Guide-to-Deep-Learning-Semantic-Segmentation" class="headerlink" title="A Theory and Practical Guide to Deep Learning Semantic Segmentation"></a>A Theory and Practical Guide to Deep Learning Semantic Segmentation</h1><center><p><strong>v1.0 louwill</strong><br><br><strong>Machine Learning Lab</strong></p></center><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/qrcode.jpg" width="300" height="300"><br><div style="color:#999;font-size:11px;padding:2px">Fig0. Machine Learning Lab</div></center><br><br><br><div style="text-align:justify"><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>图像分类、目标检测和图像分割是基于深度学习的计算机视觉三大核心任务。三大任务之间明显存在着一种递进的层级关系，图像分类聚焦于整张图像，目标检测定位于图像具体区域，而图像分割则是细化到每一个像素。基于深度学习的图像分割具体包括语义分割、实例分割和全景分割。语义分割的目的是要给每个像素赋予一个语义标签。语义分割在自动驾驶、场景解析、卫星遥感图像和医学影像等领域都有着广泛的应用前景。本文作为基于PyTorch的语义分割技术手册，对语义分割的基本技术框架、主要网络模型和技术方法提供一个实战性指导和参考。</p><h2 id="1-语义分割概述"><a href="#1-语义分割概述" class="headerlink" title="1. 语义分割概述"></a>1. 语义分割概述</h2><p>图像分割主要包括语义分割（Semantic Segmentation）和实例分割（Instance Segmentation）。那语义分割和实例分割具体都是什么含义？二者又有什么区别和联系？语义分割是对图像中的每个像素都划分出对应的类别，即实现像素级别的分类；而类的具体对象，即为实例，那么实例分割不但要进行像素级别的分类，还需在具体的类别基础上区别开不同的个体。例如，图像有多个人甲、乙、丙，那边他们的语义分割结果都是人，而实例分割结果却是不同的对象。另外，为了同时实现实例分割与不可数类别的语义分割，相关研究又提出了全景分割（Panoptic Segmentation）的概念。语义分割、实例分割和全景分割具体如图1（b）、（c）和（d）图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic__1.png" width="400" height="300"><br><div style="color:#999;font-size:11px;padding:2px">Fig1. Image Segmentation</div></center><br>在开始图像分割的学习和尝试之前，我们必须明确语义分割的任务描述，即搞清楚语义分割的输入输出都是什么。输入是一张原始的RGB图像或者单通道图像，但是输出不再是简单的分类类别或者目标定位，而是带有各个像素类别标签的与输入同分辨率的分割图像。简单来说，我们的输入输出都是图像，而且是同样大小的图像。如图2所示。<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-17-at-9.02.15-PM.png" width="500" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig2. Pixel Representation</div></center><br>类似于处理分类标签数据，对预测分类目标采用像素上的one-hot编码，即为每个分类类别创建一个输出的通道。如图3所示。<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-16-at-9.36.00-PM.png" width="500" height="250"><br><div style="color:#999;font-size:11px;padding:2px">Fig3. Pixel One-hot</div></center><br>图4是将分割图添加到原始图像上的叠加效果。这里需要明确一下mask的概念，在图像处理中我们将其译为掩码，如Mask R-CNN中的Mask。Mask可以理解为我们将预测结果叠加到单个通道时得到的该分类所在区域。<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-16-at-9.36.38-PM.png" width="500" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig4. Pixel labeling</div></center><br>所以，语义分割的任务就是输入图像经过深度学习算法处理得到带有语义标签的同样尺寸的输出图像。<h2 id="2-关键技术组件"><a href="#2-关键技术组件" class="headerlink" title="2. 关键技术组件"></a>2. 关键技术组件</h2><p>在语义分割发展早期，为了能够让深度学习进行像素级的分类任务，在分类任务的基础上对CNN做了一些修改，将分类网络中浓缩语义表征的全连接层去掉，提出用全卷积网络（Fully Convolutional Networks）来处理语义分割问题。然后U-Net的提出，奠定了编解码结构的U形网络深度学习语义分割中的总统山地位。这里我们对语义分割的关键技术组件进行分开描述，编码器、解码器和Skip Connection属于分割网络的核心结构组件，空洞卷积（Dilate Conv）是独立于U形结构的第二大核心设计。条件随机场（CRF）和马尔科夫随机场（MRF）则是用于优化神经网络分割后的细节处理。深监督作为一种常用的结构设计Trick，在分割网络中也有广泛应用。除此之外，则是针对于语义分割的通用技术点。</p><h3 id="2-1-编码器与分类网络"><a href="#2-1-编码器与分类网络" class="headerlink" title="2.1 编码器与分类网络"></a>2.1 编码器与分类网络</h3><p>编码器对于分割网络来说就是进行特征提取和语义信息浓缩的过程，这对熟悉各种分类网络的我们来说并不陌生。编码器通过卷积和池化的组合不断对图像进行下采样，得到的特征图空间尺寸也会越来越小，但会更加具备语义分辨性。这也是大多数分类网络的通用模式，不断卷积池化使得特征图越来越小，然后配上几层全连接网络即可进行分类判别。常用的分类网络包括AlexNet、VGG、ResNet、Inception、DenseNet和MobileNet等等。<br><br></p><p>既然之前有那么多优秀的SOTA网络用来做特征提取，所以很多时候分割网络的编码器并不需要我们write from scratch，时刻要有迁移学习的敏感度，直接用现成分类网络的卷积层部分作为编码器进行特征提取和信息浓缩，往往要比从头开始训练一个编码器要快很多。<br><br></p><p>比如我们以VGG16作为SegNet编码器的预训练模型，以PyTorch为例，来看编码器的写法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SegNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, classes</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        vgg16 = models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">        features = vgg16.features</span><br><span class="line">        self.enc1 = features[<span class="number">0</span>: <span class="number">4</span>]</span><br><span class="line">        self.enc2 = features[<span class="number">5</span>: <span class="number">9</span>]</span><br><span class="line">        self.enc3 = features[<span class="number">10</span>: <span class="number">16</span>]</span><br><span class="line">        self.enc4 = features[<span class="number">17</span>: <span class="number">23</span>]</span><br><span class="line">        self.enc5 = features[<span class="number">24</span>: -<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>在上述代码中，可以看到我们将vgg16的31个层分作5个编码模块，每个编码模块的基本结构如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))      </span><br><span class="line">(<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)                                          </span><br><span class="line">(<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))     </span><br><span class="line">(<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)                                          </span><br><span class="line">(<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)   </span><br></pre></td></tr></table></figure><h3 id="2-2-解码器与上采样"><a href="#2-2-解码器与上采样" class="headerlink" title="2.2 解码器与上采样"></a>2.2 解码器与上采样</h3><p>编码器不断将输入不断进行下采样达到信息浓缩，而解码器则负责上采样来恢复输入尺寸。解码器中除了一些卷积方法用为辅助之外，最关键的还是一些上采样方法，主要包括双线性插值、转置卷积和反池化。</p><h4 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h4><p>插值法（Interpolation）是一种经典的数值分析方法，一些经典插值大家或多或少都有听到过，比如线性插值、三次样条插值和拉格朗日插值法等。在说双线性插值前我们先来了解一下什么是线性插值（Linear interpolation）。线性插值法是指使用连接两个已知量的直线来确定在这两个已知量之间的一个未知量的值的方法。如下图所示：</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_2.png" width="400" height="250"><br><div style="color:#999;font-size:11px;padding:2px">Fig5. Linear Interpolation</div></center><br><p>已知直线上两点坐标分别为$(x_1,y_1)$和$(x_2,y_2)$，现在想要通过线性插值法来得到某一点$x$在直线上的值。基本就是一个初中数学题，这里就不做过多展开，点$x$在直线上的值$y$可以表示为：</p><center><p>$y&#x3D;\frac{x_2-x}{x_2-x_1}y_2+\frac{x-x_1}{x_2-x_1}y_1$</p></center><p>再来看双线性插值。线性插值用到两个点来确定插值，双线性插值则需要四个点。在图像上采样中，双线性插值利用四个点的像素值来确定要插值的一个像素值，其本质上还是分别在$x$和$y$方向上分别进行两次线性插值。如下图所示，我们来看具体做法。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_3.png" width="400" height="250"><br><div style="color:#999;font-size:11px;padding:2px">Fig6. Bilinear Interpolation</div></center><br><p>图中$Q_{11}-Q_{22}$四个黄色的点是已知数据点，红色点$P$是待插值点。假设$Q_{11}$为$(x_1,y_1)$，$Q_{12}$为$(x_1,y_2)$，$Q_{21}$为$(x_2,y_1)$，$Q_{22}$为$(x_2,y_2)$。我们先在$x$轴方向上进行线性插值，先求得$R_1$和$R_2$的插值。根据线性插值公式，有：</p><center><p>$f(R_1)&#x3D;\frac{x_2-x}{x_2-x_1}f(Q_{11})+\frac{x-x_1}{x_2-x_1}f(Q_{21})$<br>$f(R_2)&#x3D;\frac{x_2-x}{x_2-x_1}f(Q_{12})+\frac{x-x_1}{x_2-x_1}f(Q_{22})$</p></center><p>得到$R_1$和$R_2$点坐标之后，便可继续在$y$轴方向进行线性插值。可得目标点$P$的插值为：</p><center><p>$f(P)&#x3D;\frac{y_2-y}{y_2-y_1}f(R_1)+\frac{y-y_1}{y_2-y_1}f(R_2)$</p></center><p>双线性插值在众多经典的语义分割网络中都有用到，比如说奠定语义分割编解码框架的FCN网络。假设将$3\times6$的图像通过双线性插值变为$6\times12$的图像，如下图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_4.jpg" width="500" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig7. Bilinear Interpolation Example</div></center><br>双线性插值的优点是速度非常快，计算量小，但缺点就是效果不是特别理想。<br><h4 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h4><p>转置卷积（Transposed Convolution）也叫解卷积（Deconvolution），有些人也将其称为反卷积，但这个叫法并不太准确。大家都知道，在常规卷积时，我们每次得到的卷积特征图尺寸是越来越小的。但在图像分割等领域，我们是需要逐步恢复输入时的尺寸的。如果把常规卷积时的特征图不断变小叫做下采样，那么通过转置卷积来恢复分辨率的操作可以称作上采样。</p><p>本质上来说，转置卷积跟常规卷积并无区别。不同之处在于先按照一定的比例进行padding来扩大输入尺寸，然后把常规卷积中的卷积核进行转置，再按常规卷积方法进行卷积就是转置卷积。假设输入图像矩阵为$X$，卷积核矩阵为$C$，常规卷积的输出为$Y$，则有：</p><center><p>$Y&#x3D;CX$</p></center><p>两边同时乘以卷积核的转置$C^T$，这个公式便是转置卷积的输入输出计算。</p><center><p>$X&#x3D;C^TY$</p></center><p>假设输入大小为$4\times4$，滤波器大小为$3\times3$，常规卷积下输出为$2\times2$，为了演示转置卷积，我们将滤波器矩阵进行稀疏化处理为$4\times16$，将输入矩阵进行拉平为$16\times1$，相应输出结果也会拉平为$4\times1$，图示如下：</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://miro.medium.com/max/2307/1*9ngOwG-uHaJO8Od0ePB-fQ.jpeg" width="380" height="310"><br><div style="color:#999;font-size:11px;padding:2px">Fig8. Matrix of Convolution</div></center><p>然后按照转置卷积的做法我们把卷积核矩阵进行转置，按照$X&#x3D;C^TY$进行验证：</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://miro.medium.com/max/2350/1*zfIgQ6uyowDUhkMBBKh4bg.png" width="350" height="300"><br><div style="color:#999;font-size:11px;padding:2px">Fig9. Matrix of Transpose Convolution</div></center><h4 id="反池化"><a href="#反池化" class="headerlink" title="反池化"></a>反池化</h4><p>反池化（Unpooling）可以理解为池化的逆操作，相较于前两种上采样方法，反池化用的并不是特别多。其简要原理如下，在池化时记录下对应kernel中的坐标，在反池化时将一个元素根据kernel进行放大，根据之前的坐标将元素填写进去，其他位置补位为0即可。</p><h3 id="2-3-Skip-Connection"><a href="#2-3-Skip-Connection" class="headerlink" title="2.3 Skip Connection"></a>2.3 Skip Connection</h3><p>跳跃连接本身是在ResNet中率先提出，用于学习一个恒等式和残差结构，后面在DenseNet、FCN和U-Net等网络中广泛使用。最典型的就是U-Net的跳跃连接，在每个编码和解码层之间各添加一个跳跃连接，每一次下采样都会有一个跳跃连接与对应的上采样进行级联，这种不同尺度的特征融合对上采样恢复像素大有帮助。</p><h3 id="2-4-Dilate-Conv与多尺度"><a href="#2-4-Dilate-Conv与多尺度" class="headerlink" title="2.4 Dilate Conv与多尺度"></a>2.4 Dilate Conv与多尺度</h3><p>空洞卷积（Dilated&#x2F;Atrous Convolution）也叫扩张卷积或者膨胀卷积，字面意思上来说就是在卷积核中插入空洞，起到扩大感受野的作用。空洞卷积的直接做法是在常规卷积核中填充0，用来扩大感受野，且进行计算时，空洞卷积中实际只有非零的元素起了作用。假设以一个变量a来衡量空洞卷积的扩张系数，则加入空洞之后的实际卷积核尺寸与原始卷积核尺寸之间的关系：</p><center><p>$K&#x3D;k+(k-1)(a-1)$</p></center><p>其中$k$为原始卷积核大小，$a$为卷积扩张率（dilation rate），$K$为经过扩展后实际卷积核大小。除此之外，空洞卷积的卷积方式跟常规卷积一样。当$a&#x3D;1$时，空洞卷积就退化为常规卷积。$a&#x3D;1,2,4$时，空洞卷积示意图如下：</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_9.png" width="600" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig10. Dialate Convolution</div></center><div style="text-align:justify"><p>当$a&#x3D;1$，原始卷积核size为$3\times3$，就是常规卷积。$a&#x3D;2$时，加入空洞之后的卷积核$size&#x3D;3+(3-1)\times(2-1)&#x3D;5$，对应的感受野可计算为$2^{(a+2)}-1&#x3D;7$。$a&#x3D;3$时，卷积核size可以变化到$3+(3-1)(4-1)&#x3D;9$，感受野则增长到$2^{(a+2)}-1&#x3D;15$。对比不加空洞卷积的情况，在stride为1的情况下3层3x3卷积的叠加，第三层输出特征图对应的感受野也只有$1+(3-1)\times3&#x3D;7$。所以，空洞卷积的一个重要作用就是增大感受野。</p><p>在语义分割的发展历程中，增大感受野是一个非常重要的设计。早期FCN提出以全卷积方式来处理像素级别的分割任务时，包括后来奠定语义分割baseline地位的U-Net，网络结构中存在大量的池化层来进行下采样，大量使用池化层的结果就是损失掉了一些信息，在解码上采样重建分辨率的时候肯定会有影响。特别是对于多目标、小物体的语义分割问题，以U-Net为代表的分割模型一直存在着精度瓶颈的问题。而基于增大感受野的动机背景下就提出了以空洞卷积为重大创新的deeplab系列分割网络，我们在深度学习语义分割模型中会对deeplab进行详述，这里不做过多展开。</p><p>对于语义分割而言，空洞卷积主要有三个作用：</p><ul><li><p>第一是扩大感受野，具体前面已经说的比较多了，这里不做重复。但需要明确一点，池化也可以扩大感受野，但空间分辨率降低了，相比之下，空洞卷积可以在扩大感受野的同时不丢失分辨率，且保持像素的相对空间位置不变。简单而言就是空洞卷积可以同时控制感受野和分辨率。</p></li><li><p>第二就是获取多尺度上下文信息。当多个带有不同dilation rate的空洞卷积核叠加时，不同的感受野会带来多尺度信息，这对于分割任务是非常重要的。</p></li><li><p>第三就是可以降低计算量，不需要引入额外的参数，如上图空洞卷积示意图所示，实际卷积时只有带有红点的元素真正进行计算。</p></li></ul></div><h3 id="2-5-后处理技术"><a href="#2-5-后处理技术" class="headerlink" title="2.5 后处理技术"></a>2.5 后处理技术</h3><div style="text-align:justify">早期语义分割模型效果较为粗糙，在没有更好的特征提取模型的情况下，研究者们便在神经网络模型的粗糙结果进行后处理（Post-Processing），主要方法就是一些常用的概率图模型，比如说条件随机场（Conditional Random Field,CRF）和马尔可夫随机场（Markov Random Field,MRF）。<p>CRF是一种经典的概率图模型，简单而言就是给定一组输入序列的条件下，求另一组输出序列的条件概率分布模型，CRF在自然语言处理领域有着广泛应用。CRF在语义分割后处理中用法的基本思路如下：对于FCN或者其他分割网络的粗粒度分割结果而言，每个像素点$i$具有对应的类别标签$x_i$和观测值$y_i$，以每个像素为节点，以像素与像素之间的关系作为边即可构建一个CRF模型。在这个CRF模型中，我们通过观测变量$y_i$来预测像素$i$对应的标签值$x_i$。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_11.jpg" width="250" height="180"><br><div style="color:#999;font-size:11px;padding:2px">Fig11. CRF</div></center><p>以上做法也叫DenseCRF，具体细节可参考论文：<br><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</a>，除此之外还有CRFasRNN，采用平均场近似的方式将CRF方法融入到神经网络过程中，本质上是对DenseCRF的一种优化。</p><p>另一种后处理概率图模型是MRF，MRF与CRF较为类似，只是对CRF的二元势函数做了调整，其优点在于可以使用平均场来构造CNN网络，并且推理过程可以一次性搞定。MRF在Deep Parsing Network(DPN)中有详细描述，相关细节可参考论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1509.02634.pdf">Semantic Image Segmentation via Deep Parsing Network</a>。</p><p>语义分割发展前期，在分割网络模型的结果上加上CRF和MRF等后处理技术形成了早期的语义分割技术框架：</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_10.jpg" width="500" height="250"><br><div style="color:#999;font-size:11px;padding:2px">Fig12. Framework of Semantic Segmentation with CRF/MRF</div></center><p>但从Deeplab v3开始，主流的语义分割网络就不再热衷于后处理技术了。一个典型的观点认为神经网络分割效果不好才会用后处理技术，这说明在分割网络本身上还有很大的提升空间。一是CRF本身不太容易训练，二来语义分割任务的端到端趋势。后来语义分割领域的SOTA网络也确实证明了这一点。尽管如此，CRF等后处理技术作为语义分割发展历程上的一个重要方法，我们有必要在此进行说明。从另一方面看，深度学习和概率图的结合虽然并不是那么顺利，但相信未来依旧会大有前景。</p></div><h3 id="2-6-深监督"><a href="#2-6-深监督" class="headerlink" title="2.6 深监督"></a>2.6 深监督</h3><p>所谓深监督（Deep Supervision），就是在深度神经网络的某些中间隐藏层加了一个辅助的分类器作为一种网络分支来对主干网络进行监督的技巧，用来解决深度神经网络训练梯度消失和收敛速度过慢等问题。</p><p>带有深监督的一个8层深度卷积网络结构如下图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_5.png" width="600" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig13. Deep Supervision Example</div></center><br><p>可以看到，图中在第四个卷积块之后添加了一个监督分类器作为分支。<code>Conv4</code>输出的特征图除了随着主网络进入<code>Conv5</code>之外，也作为输入进入了分支分类器。如图所示，该分支分类器包括一个卷积块、两个带有<code>Dropout</code>和<code>ReLu</code>的全连接块和一个纯全连接块。带有深监督的卷积模块例子如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">C1DeepSup</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_class=<span class="number">150</span>, fc_dim=<span class="number">2048</span>, use_softmax=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(C1DeepSup, self).__init__()</span><br><span class="line">        self.use_softmax = use_softmax</span><br><span class="line">        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // <span class="number">2</span>, fc_dim // <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后一层卷积</span></span><br><span class="line">        self.conv_last = nn.Conv2d(fc_dim // <span class="number">4</span>, num_class, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.conv_last_deepsup = nn.Conv2d(fc_dim // <span class="number">4</span>, num_class, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向计算流程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, conv_out, segSize=<span class="literal">None</span></span>):</span><br><span class="line">        conv5 = conv_out[-<span class="number">1</span>]</span><br><span class="line">        x = self.cbr(conv5)</span><br><span class="line">        x = self.conv_last(x)</span><br><span class="line">        <span class="comment"># is True during inference</span></span><br><span class="line">        <span class="keyword">if</span> self.use_softmax:  </span><br><span class="line">            x = nn.functional.interpolate(</span><br><span class="line">                x, size=segSize, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">            x = nn.functional.softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">        <span class="comment"># 深监督模块</span></span><br><span class="line">        conv4 = conv_out[-<span class="number">2</span>]</span><br><span class="line">        _ = self.cbr_deepsup(conv4)</span><br><span class="line">        _ = self.conv_last_deepsup(_)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 主干卷积网络softmax输出</span></span><br><span class="line">        x = nn.functional.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 深监督分支网络softmax输出</span></span><br><span class="line">        _ = nn.functional.log_softmax(_, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> (x, _)</span><br></pre></td></tr></table></figure><h3 id="2-7-通用技术"><a href="#2-7-通用技术" class="headerlink" title="2.7 通用技术"></a>2.7 通用技术</h3><p>通用技术主要是指深度学习流程中会用到的基本模块，比如说损失函数的选取以及采用哪种精度衡量指标。其他的像优化器的选取，学习率的控制等，这里限于篇幅进行省略。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>常用的分类损失均可用作语义分割的损失函数。最常用的就是交叉熵损失函数，如果只是前景分割，则可以使用二分类的交叉熵损失（Binary CrossEntropy Loss, BCE loss），对于目标物体较小的情况我们可以使用Dice损失，对于目标物体类别不均衡的情况可以使用加权的交叉熵损失（Weighted CrossEntropy Loss, WCE Loss），另外也可以尝试多种损失函数的组合。</p><h4 id="精度描述"><a href="#精度描述" class="headerlink" title="精度描述"></a>精度描述</h4><p>语义分割作为经典的图像分割问题，其本质上还是一种图像像素分类。既然是分类，我们就可以使用常见的分类评价指标来评估模型好坏。语义分割常见的评价指标包括像素准确率（Pixel Accuracy）、平均像素准确率（Mean Pixel Accuracy）、平均交并比（Mean IoU）、频权交并比（FWIoU）和Dice系数（Dice Coeffcient）等。</p><p><strong>像素准确率(PA)。</strong> 像素准确率跟分类中的准确率含义一样，即所有分类正确的像素数占全部像素的比例。PA的计算公式如下：</p><center><p>$PA&#x3D;\frac{\sum_{i&#x3D;0}^{n}p_{ii}}{\sum_{i&#x3D;0}^{n}\sum_{j&#x3D;0}^{n}p_{ij}}$</p></center><p><strong>平均像素准确率(MPA)。</strong> 平均像素准确率其实更应该叫平均像素精确率，是指分别计算每个类别分类正确的像素数占所有预测为该类别像素数比例的平均值。所以，从定义上看，这是精确率(Precision)的定义，MPA的计算公式如下：</p><center><p>$MPA&#x3D;\frac{1}{n+1}\sum_{i&#x3D;0}^{n}\frac{p_{ii}}{\sum_{j&#x3D;0}^{n}p_{ij}}$</p></center><p><strong>平均交并比(MIoU)。</strong> 交并比（Intersection over Union）的定义很简单，将标签图像和预测图像看成是两个集合，计算两个集合的交集和并集的比值。而平均交并比则是将所有类的IoU取平均。 MIoU的计算公式如下：</p><center><p>$MIoU&#x3D;\frac{1}{n+1}\sum_{i&#x3D;0}^{n}\frac{p_{ii}}{\sum_{j&#x3D;0}^{n}p_{ij}+\sum_{j&#x3D;0}^{n}p_{ji}-p_{ii}}$</p></center><p><strong>频权交并比(FWIoU)。</strong> 频权交并比顾名思义，就是以每一类别的频率为权重和其IoU加权计算出来的结果。FWIoU的设计思想很明确，语义分割很多时候会面临图像中各目标类别不平衡的情况，对各类别IoU直接求平均不是很合理，所以考虑各类别的权重就非常重要了。FWIoU的计算公式如下：</p><center><p>$FWIoU&#x3D;\frac{1}{\sum_{i&#x3D;0}^{n}\sum_{j&#x3D;0}^{n}p_{ij}}\sum_{i&#x3D;0}^{n}\frac{\sum_{j&#x3D;0}^{n}p_{ij}p_{ii}}{\sum_{j&#x3D;0}^{n}p_{ij}+\sum_{j&#x3D;0}^{n}p_{ji}-p_{ii}}$</p></center><p><strong>Dice系数。</strong> Dice系数是一种度量两个集合相似性的函数，是语义分割中最常用的评价指标之一。Dice系数定义为两倍的交集除以像素和，跟IoU有点类似，其计算公式如下：</p><center><p>$dice&#x3D;\frac{2|X\cap{Y}|}{|X|+|Y|}$</p></center><p>dice本质上跟分类指标中的F1-Score类似。作为最常用的分割指标之一，这里给出PyTorch的实现方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dice_coef</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Dice = (2*|X &amp; Y|)/ (|X|+ |Y|)</span></span><br><span class="line"><span class="string">         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    smooth = <span class="number">1.</span></span><br><span class="line">    m1 = pred.view(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">    m2 = target.view(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">    intersection = (m1 * m2).<span class="built_in">sum</span>().<span class="built_in">float</span>()</span><br><span class="line">    dice = (<span class="number">2.</span> * intersection + smooth) / (torch.<span class="built_in">sum</span>(m1*m1) + torch.<span class="built_in">sum</span>(m2*m2) + smooth)</span><br><span class="line">    <span class="keyword">return</span> dice</span><br></pre></td></tr></table></figure><h2 id="3-数据Pipeline"><a href="#3-数据Pipeline" class="headerlink" title="3. 数据Pipeline"></a>3. 数据Pipeline</h2><p>这里主要说一下PyTorch的自定义数据读取pipeline模板和相关trciks以及如何优化数据读取的pipeline等。我们从PyTorch的数据对象类<code>Dataset</code>开始。<code>Dataset</code>在PyTorch中的模块位于<code>utils.data</code>下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></table></figure><h3 id="3-1-Torch数据读取模板"><a href="#3-1-Torch数据读取模板" class="headerlink" title="3.1 Torch数据读取模板"></a>3.1 Torch数据读取模板</h3><p>PyTorch官方为我们提供了自定义数据读取的标准化代码代码模块，作为一个读取框架，我们这里称之为原始模板。其代码结构如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ...</span>):</span><br><span class="line">        <span class="comment"># stuff</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment"># stuff</span></span><br><span class="line">        <span class="keyword">return</span> (img, label)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># return examples size</span></span><br><span class="line">        <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure><h3 id="3-2-transform与数据增强"><a href="#3-2-transform与数据增强" class="headerlink" title="3.2 transform与数据增强"></a>3.2 transform与数据增强</h3><p>PyTorch数据增强功能可以放在<code>transform</code>模块下，添加<code>transform</code>后的数据读取结构如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ...</span>):</span><br><span class="line">        <span class="comment"># stuff</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># compose the transforms methods</span></span><br><span class="line">        self.transform = T.Compose([T.CenterCrop(<span class="number">100</span>),</span><br><span class="line">                                T.RandomResizedCrop(<span class="number">256</span>),</span><br><span class="line">                                T.RandomRotation(<span class="number">45</span>),</span><br><span class="line">                                T.ToTensor()])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment"># stuff</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        data = <span class="comment"># Some data read from a file or image</span></span><br><span class="line">        labe = <span class="comment"># Some data read from a file or image</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># execute the transform</span></span><br><span class="line">        data = self.transform(data)  </span><br><span class="line">        label = self.transform(label)</span><br><span class="line">        <span class="keyword">return</span> (img, label)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># return examples size</span></span><br><span class="line">        <span class="keyword">return</span> count</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># Call the dataset</span></span><br><span class="line">    custom_dataset = CustomDataset(...)</span><br></pre></td></tr></table></figure><p>需要说明的是，PyTorch <code>transform</code>模块所做的数据增强并不是我们所理解的广义上的数据增强。<code>transform</code>所做的增强，仅仅是在数据读取过程中随机地对某张图像做转化操作，实际数据量上并没有增多，可以将其视为是一种在线增强的策略。如果想要实现实际训练数据成倍数的增加，可以使用离线增强策略。</p><p>与图像分类仅需要对输入图像做增强不同的是，对于语义分割的数据增强而言，需要同时对输入图像和输入的mask同步进行数据增强工作。实际写代码时，要记得使用随机种子，在不失随机性的同时，保证输入图像和输出mask具备同样的转换。一个完整的语义分割在线数据增强代码实例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SegmentationDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># read the input images</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_input_image</span>(<span class="params">path</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            img = Image.<span class="built_in">open</span>(f)</span><br><span class="line">            <span class="keyword">return</span> img.convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">    <span class="comment"># read the mask images</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_target_image</span>(<span class="params">path</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            img = Image.<span class="built_in">open</span>(f)</span><br><span class="line">            <span class="keyword">return</span> img.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_root, target_root, transform_input=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 transform_target=<span class="literal">None</span>, seed_fn=<span class="literal">None</span></span>):   </span><br><span class="line">        self.input_root = input_root</span><br><span class="line">        self.target_root = target_root</span><br><span class="line">        self.transform_input = transform_input</span><br><span class="line">        self.transform_target = transform_target</span><br><span class="line">        self.seed_fn = seed_fn</span><br><span class="line">        <span class="comment"># sort the ids     </span></span><br><span class="line">        self.input_ids = <span class="built_in">sorted</span>(img <span class="keyword">for</span> img <span class="keyword">in</span> os.listdir(self.input_root))</span><br><span class="line">        self.target_ids = <span class="built_in">sorted</span>(img <span class="keyword">for</span> img <span class="keyword">in</span> os.listdir(self.target_root))</span><br><span class="line">        <span class="keyword">assert</span>(<span class="built_in">len</span>(self.input_ids) == <span class="built_in">len</span>(self.target_ids))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set random number seed</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_set_seed</span>(<span class="params">self, seed</span>):</span><br><span class="line">        random.seed(seed)</span><br><span class="line">        torch.manual_seed(seed)</span><br><span class="line">        <span class="keyword">if</span> self.seed_fn:</span><br><span class="line">            self.seed_fn(seed)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        input_img = self._load_input_image(</span><br><span class="line">            os.path.join(self.input_root, self.input_ids[idx]))</span><br><span class="line">        target_img = self._load_target_image(</span><br><span class="line">            os.path.join(self.target_root, self.target_ids[idx]))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.transform_input:</span><br><span class="line">            <span class="comment"># ensure that the input and output have the same randomness.</span></span><br><span class="line">            seed = random.randint(<span class="number">0</span>, <span class="number">2</span>**<span class="number">32</span>)</span><br><span class="line">            self._set_seed(seed)</span><br><span class="line">            input_img = self.transform_input(input_img)</span><br><span class="line">            self._set_seed(seed)</span><br><span class="line">            target_img = self.transform_target(target_img)  </span><br><span class="line">        <span class="keyword">return</span> input_img, target_img, self.input_ids[idx]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br></pre></td></tr></table></figure><p>其中<code>transform_input</code>和<code>transform_target</code>均可由<code>transform</code>模块下的函数封装而成。一个皮肤病灶分割的在线数据增强实例效果如下图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic__12.png" width="800" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig14. Example of online augmentation</div></center><h2 id="4-模型与算法"><a href="#4-模型与算法" class="headerlink" title="4. 模型与算法"></a>4. 模型与算法</h2><p>早期基于深度学习的图像分割以FCN为核心，旨在重点解决如何更好从卷积下采样中恢复丢掉的信息损失。后来逐渐形成了以U-Net为核心的这样一种编解码对称的U形结构。<strong>语义分割界迄今为止最重要的两个设计，一个是以U-Net为代表的U形结构，目前基于U-Net结构的创新就层出不穷，比如说应用于3D图像的V-Net，嵌套U-Net结构的U-Net++等。除此在外还有SegNet、RefineNet、HRNet和FastFCN。另一个则是以DeepLab系列为代表的Dilation设计，主要包括DeepLab系列和PSPNet。随着模型的Baseline效果不断提升，语义分割任务的主要矛盾也逐从downsample损失恢复像素逐渐演变为如何更有效地利用context上下文信息。</strong></p><h3 id="4-1-FCN"><a href="#4-1-FCN" class="headerlink" title="4.1 FCN"></a>4.1 FCN</h3><p>FCN（Fully Convilutional Networks）是语义分割领域的开山之作。FCN的提出是在2016年，相较于此前提出的AlexNet和VGG等卷积全连接的网络结构，FCN提出用卷积层代替全连接层来处理语义分割问题，这也是FCN的由来，即全卷积网络。</p><p>FCN的关键点主要有三，一是全卷积进行特征提取和下采样，二是双线性插值进行上采样，三是跳跃连接进行特征融合。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_6.png" width="550" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig15. FCN</div></center><p>利用PyTorch实现一个FCN-8网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchimport torch.nn <span class="keyword">as</span> nnimport torch.nn.init <span class="keyword">as</span> initimport torch.nn.functional <span class="keyword">as</span> Ffrom torch.utils <span class="keyword">import</span> model_zoofrom torchvision <span class="keyword">import</span> modelsclass FCN8(nn.Module):    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):        <span class="built_in">super</span>().__init__()        feats = <span class="built_in">list</span>(models.vgg16(pretrained=<span class="literal">True</span>).features.children())        self.feats = nn.Sequential(*feats[<span class="number">0</span>:<span class="number">9</span>])        self.feat3 = nn.Sequential(*feats[<span class="number">10</span>:<span class="number">16</span>])        self.feat4 = nn.Sequential(*feats[<span class="number">17</span>:<span class="number">23</span>])        self.feat5 = nn.Sequential(*feats[<span class="number">24</span>:<span class="number">30</span>])        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):                m.requires_grad = <span class="literal">False</span>        self.fconn = nn.Sequential(            nn.Conv2d(<span class="number">512</span>, <span class="number">4096</span>, <span class="number">7</span>),            nn.ReLU(inplace=<span class="literal">True</span>),            nn.Dropout(),            nn.Conv2d(<span class="number">4096</span>, <span class="number">4096</span>, <span class="number">1</span>),            nn.ReLU(inplace=<span class="literal">True</span>),            nn.Dropout(),        )        self.score_feat3 = nn.Conv2d(<span class="number">256</span>, num_classes, <span class="number">1</span>)        self.score_feat4 = nn.Conv2d(<span class="number">512</span>, num_classes, <span class="number">1</span>)        self.score_fconn = nn.Conv2d(<span class="number">4096</span>, num_classes, <span class="number">1</span>)    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):        feats = self.feats(x)        feat3 = self.feat3(feats)        feat4 = self.feat4(feat3)        feat5 = self.feat5(feat4)        fconn = self.fconn(feat5)        score_feat3 = self.score_feat3(feat3)        score_feat4 = self.score_feat4(feat4)        score_fconn = self.score_fconn(fconn)        score = F.upsample_bilinear(score_fconn, score_feat4.size()[<span class="number">2</span>:])        score += score_feat4        score = F.upsample_bilinear(score, score_feat3.size()[<span class="number">2</span>:])        score += score_feat3        <span class="keyword">return</span> F.upsample_bilinear(score, x.size()[<span class="number">2</span>:])</span><br></pre></td></tr></table></figure><p>从代码中可以看到，我们使用了vgg16作为FCN-8的编码部分，这使得FCN-8具备较强的特征提取能力。</p><h3 id="4-2-UNet"><a href="#4-2-UNet" class="headerlink" title="4.2 UNet"></a>4.2 UNet</h3><p>早期基于深度学习的图像分割以FCN为核心，旨在重点解决如何更好从卷积下采样中恢复丢掉的信息损失。后来逐渐形成了以UNet为核心的这样一种编解码对称的U形结构。</p><p>UNet结构能够在分割界具有一统之势，最根本的还是其效果好，尤其是在医学图像领域。所以，做医学影像相关的深度学习应用时，一定都用过UNet，而且最原始的UNet一般都会有一个不错的baseline表现。2015年发表UNet的MICCAI，是目前医学图像分析领域最顶级的国际会议，UNet为什么在医学上效果这么好非常值得探讨一番。</p><p>U-Net结构如下图所示：</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_7.png" width="500" height="300"><br><div style="color:#999;font-size:11px;padding:2px">Fig16. UNet</div></center><br>乍一看很复杂，U形结构下貌似有很多细节问题。我们来把UNet简化一下，如下图所示：<center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_8.png" width="500" height="300"><br><div style="color:#999;font-size:11px;padding:2px">Fig17. U-Net简化</div></center><br>从图中可以看到，简化之后的UNet的关键点只有三条线：<ul><li>下采样编码</li><li>上采样解码</li><li>跳跃连接</li></ul><p>下采样进行信息浓缩和上采样进行像素恢复，这是其他分割网络都会有的部分，UNet自然也不会跳出这个框架，可以看到，UNet进行了4次的最大池化下采样，每一次采样后都使用了卷积进行信息提取得到特征图，然后再经过4次上采样恢复输入像素尺寸。但UNet最关键的、也是最特色的部分在于图中红色虚线的Skip Connection。每一次下采样都会有一个跳跃连接与对应的上采样进行级联，这种不同尺度的特征融合对上采样恢复像素大有帮助，具体来说就是高层（浅层）下采样倍数小，特征图具备更加细致的图特征，底层（深层）下采样倍数大，信息经过大量浓缩，空间损失大，但有助于目标区域（分类）判断，当high level和low level的特征进行融合时，分割效果往往会非常好。从某种程度上讲，这种跳跃连接也可以视为一种Deep Supervision。</p><p>U-Net的简单实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码块class UNetEnc(nn.Module):    def __init__(self, in_channels, out_channels, dropout=False):        super().__init__()        layers = [            nn.Conv2d(in_channels, out_channels, 3, dilation=2),            nn.ReLU(inplace=True),            nn.Conv2d(out_channels, out_channels, 3, dilation=2),            nn.ReLU(inplace=True),        ]        if dropout:            layers += [nn.Dropout(.5)]        layers += [nn.MaxPool2d(2, stride=2, ceil_mode=True)]        self.down = nn.Sequential(*layers)    def forward(self, x):        return self.down(x)# 解码块		class UNetDec(nn.Module):    def __init__(self, in_channels, features, out_channels):        super().__init__()        self.up = nn.Sequential(            nn.Conv2d(in_channels, features, 3),            nn.ReLU(inplace=True),            nn.Conv2d(features, features, 3),            nn.ReLU(inplace=True),            nn.ConvTranspose2d(features, out_channels, 2, stride=2),            nn.ReLU(inplace=True),        )    def forward(self, x):        return self.up(x)# U-Netclass UNet(nn.Module):    def __init__(self, num_classes):        super().__init__()        self.enc1 = UNetEnc(3, 64)        self.enc2 = UNetEnc(64, 128)        self.enc3 = UNetEnc(128, 256)        self.enc4 = UNetEnc(256, 512, dropout=True)        self.center = nn.Sequential(            nn.Conv2d(512, 1024, 3),            nn.ReLU(inplace=True),            nn.Conv2d(1024, 1024, 3),            nn.ReLU(inplace=True),            nn.Dropout(),            nn.ConvTranspose2d(1024, 512, 2, stride=2),            nn.ReLU(inplace=True),        )        self.dec4 = UNetDec(1024, 512, 256)        self.dec3 = UNetDec(512, 256, 128)        self.dec2 = UNetDec(256, 128, 64)        self.dec1 = nn.Sequential(            nn.Conv2d(128, 64, 3),            nn.ReLU(inplace=True),            nn.Conv2d(64, 64, 3),            nn.ReLU(inplace=True),        )        self.final = nn.Conv2d(64, num_classes, 1)    # 前向传播过程    def forward(self, x):        enc1 = self.enc1(x)        enc2 = self.enc2(enc1)        enc3 = self.enc3(enc2)        enc4 = self.enc4(enc3)        center = self.center(enc4)        # 包含了同层分辨率级联的解码块        dec4 = self.dec4(torch.cat([            center, F.upsample_bilinear(enc4, center.size()[2:])], 1))        dec3 = self.dec3(torch.cat([            dec4, F.upsample_bilinear(enc3, dec4.size()[2:])], 1))        dec2 = self.dec2(torch.cat([            dec3, F.upsample_bilinear(enc2, dec3.size()[2:])], 1))        dec1 = self.dec1(torch.cat([            dec2, F.upsample_bilinear(enc1, dec2.size()[2:])], 1))                return F.upsample_bilinear(self.final(dec1), x.size()[2:])</span></span><br></pre></td></tr></table></figure><h3 id="4-3-SegNet"><a href="#4-3-SegNet" class="headerlink" title="4.3 SegNet"></a>4.3 SegNet</h3><p>SegNet网络是典型的编码-解码结构。SegNet编码器网络由VGG16的前13个卷积层构成，所以通常是使用VGG16的预训练权重来进行初始化。每个编码器层都有一个对应的解码器层，因此解码器层也有13层。编码器最后的输出输入到softmax分类器中，输出每个像素的类别概率。SegNet如下图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_13.png" width="600" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig18. SegNet结构</div></center><p>SegNet的一个简易参考实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchimport torch.nn <span class="keyword">as</span> nnimport torch.nn.init <span class="keyword">as</span> initimport torch.nn.functional <span class="keyword">as</span> Ffrom torchvision <span class="keyword">import</span> models<span class="comment"># define Decoderclass SegNetDec(nn.Module):    def __init__(self, in_channels, out_channels, num_layers):        super().__init__()        layers = [            nn.Conv2d(in_channels, in_channels // 2, 3, padding=1),            nn.BatchNorm2d(in_channels // 2),            nn.ReLU(inplace=True),        ]        layers += [            nn.Conv2d(in_channels // 2, in_channels // 2, 3, padding=1),            nn.BatchNorm2d(in_channels // 2),            nn.ReLU(inplace=True),        ] * num_layers        layers += [            nn.Conv2d(in_channels // 2, out_channels, 3, padding=1),            nn.BatchNorm2d(out_channels),            nn.ReLU(inplace=True),        ]        self.decode = nn.Sequential(*layers)    def forward(self, x):        return self.decode(x)# SegNetclass SegNet(nn.Module):    def __init__(self, classes):        super().__init__()        vgg16 = models.vgg16(pretrained=True)        features = vgg16.features        self.enc1 = features[0: 4]        self.enc2 = features[5: 9]        self.enc3 = features[10: 16]        self.enc4 = features[17: 23]        self.enc5 = features[24: -1]        for m in self.modules():            if isinstance(m, nn.Conv2d):                m.requires_grad = False        self.dec5 = SegNetDec(512, 512, 1)        self.dec4 = SegNetDec(512, 256, 1)        self.dec3 = SegNetDec(256, 128, 1)        self.dec2 = SegNetDec(128, 64, 0)        self.final = nn.Sequential(*[            nn.Conv2d(64, classes, 3, padding=1),            nn.BatchNorm2d(classes),            nn.ReLU(inplace=True)        ])    def forward(self, x):        x1 = self.enc1(x)        e1, m1 = F.max_pool2d(x1, kernel_size=2, stride=2, return_indices=True)        x2 = self.enc2(e1)        e2, m2 = F.max_pool2d(x2, kernel_size=2, stride=2, return_indices=True)        x3 = self.enc3(e2)        e3, m3 = F.max_pool2d(x3, kernel_size=2, stride=2, return_indices=True)        x4 = self.enc4(e3)        e4, m4 = F.max_pool2d(x4, kernel_size=2, stride=2, return_indices=True)        x5 = self.enc5(e4)        e5, m5 = F.max_pool2d(x5, kernel_size=2, stride=2, return_indices=True)        def upsample(d):            d5 = self.dec5(F.max_unpool2d(d, m5, kernel_size=2, stride=2, output_size=x5.size()))            d4 = self.dec4(F.max_unpool2d(d5, m4, kernel_size=2, stride=2, output_size=x4.size()))            d3 = self.dec3(F.max_unpool2d(d4, m3, kernel_size=2, stride=2, output_size=x3.size()))            d2 = self.dec2(F.max_unpool2d(d3, m2, kernel_size=2, stride=2, output_size=x2.size()))            d1 = F.max_unpool2d(d2, m1, kernel_size=2, stride=2, output_size=x1.size())            return d1        d = upsample(e5)        return self.final(d)</span></span><br></pre></td></tr></table></figure><h3 id="4-4-Deeplab系列"><a href="#4-4-Deeplab系列" class="headerlink" title="4.4 Deeplab系列"></a>4.4 Deeplab系列</h3><p>Deeplab系列可以算是深度学习语义分割的另一个主要架构，其代表方法就是基于Dilation的多尺度设计。Deeplab系列主要包括：</p><ul><li>Deeplab v1</li><li>Deeplab v2</li><li>Deeplab v3</li><li>Deeplab v3+</li></ul><p>Deeplab v1主要是率先使用了空洞卷积，是Deeplab系列最原始的版本。Deeplab v2在Deeplab v1的基础上最大的改进在于提出了ASPP（Atrous Spatial Pyramid Pooling），即带有空洞卷积的金字塔池化，该设计的主要目的就是提取图像的多尺度特征。另外Deeplab v2也将Deeplab v1的Backone网络更换为ResNet。Deeplab v1和v2还有一个比较大的特点就是使用了CRF作为后处理技术。</p><p>这里重点说一下多尺度问题。多尺度问题就是当图像中的目标对象存在不同大小时，分割效果不佳的现象。比如同样的物体，在近处拍摄时物体显得大，远处拍摄时显得小。解决多尺度问题的目标就是不论目标对象是大还是小，网络都能将其分割地很好。Deeplab v2使用ASPP处理多尺度问题，ASPP设计结构如下图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_14.png" width="500" height="300"><br><div style="color:#999;font-size:11px;padding:2px">Fig19. ASPP</div></center><p>从Deeplab v3开始，Deeplab系列舍弃了CRF后处理模块，提出了更加通用的、适用任何网络的分割框架，对ResNet最后的Block做了复制和级联（Cascade），对ASPP模块做了升级，在其中添加了BN层。改进后的ASPP如下图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/1594285975(1).png" width="1000" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig20. ASPP of Deeplab v3</div></center><p>Deeplab v3+在Deeplab v3的基础上做了扩展和改进，其主要改进就是在编解码结构上使用了ASPP。Deeplab v3+可以视作是融合了语义分割两大流派的一项工作，即编解码+ASPP结构。另外Deeplab v3+的Backbone换成了Xception，其深度可分离卷积的设计使得分割网络更加高效。Deeplab v3+结构如下图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_15.png" width="500" height="250"><br><div style="color:#999;font-size:11px;padding:2px">Fig21. ASPP</div></center><p>关于Deeplab系列各个版本的技术点构成总结如下表所示。Deeplab系列算法实现可参考GitHub上各版本，这里不再一一给出。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_16.png" width="1000" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig22. Summary of Deeplab series.</div></center><h3 id="4-5-PSPNet"><a href="#4-5-PSPNet" class="headerlink" title="4.5 PSPNet"></a>4.5 PSPNet</h3><p>PSPNet是针对多尺度问题提出的另一种代表性分割网络。PSPNet认为此前的分割网络没有引入足够的上下文信息及不同感受野下的全局信息而存在分割出现错误的情况，因而引入Global-Scence-Level的信息解决该问题，其Backbone网络也是ResNet。简单来说，PSPNet就是将Deeplab的ASPP模块之前的特征图Pooling了四种尺度，然后将原始特征图和四种Pooling之后的特征图进行合并到一起，再经过一系列卷积之后进行预测的过程。PSPNet结构如下图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_17.png" width="800" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig23. PSPNet</div></center><p>一个简易的带有深监督的PSPNet PPM模块写法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pyramid Pooling Module class PPMDeepsup(nn.Module):    def __init__(self, num_class=150, fc_dim=4096,                 use_softmax=False, pool_scales=(1, 2, 3, 6)):        super(PPMDeepsup, self).__init__()        self.use_softmax = use_softmax        # PPM        self.ppm = []        for scale in pool_scales:            self.ppm.append(nn.Sequential(                nn.AdaptiveAvgPool2d(scale),                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),                BatchNorm2d(512),                nn.ReLU(inplace=True)            ))        self.ppm = nn.ModuleList(self.ppm)        # Deep Supervision        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)        self.conv_last = nn.Sequential(            nn.Conv2d(fc_dim+len(pool_scales)*512, 512,                      kernel_size=3, padding=1, bias=False),            BatchNorm2d(512),            nn.ReLU(inplace=True),            nn.Dropout2d(0.1),            nn.Conv2d(512, num_class, kernel_size=1)        )        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)        self.dropout_deepsup = nn.Dropout2d(0.1)</span></span><br></pre></td></tr></table></figure><h3 id="4-6-UNet"><a href="#4-6-UNet" class="headerlink" title="4.6 UNet++"></a>4.6 UNet++</h3><p>自从2015年UNet网络提出后，这么多年大家没少在这个U形结构上折腾。大部分做语义分割的朋友都没少在UNet结构上做各种魔改，如果把UNet++算作是UNet的一种魔改的话，那它一定是最成功的魔改者。</p><p>UNet++是一种嵌套的U-Net结构，即内置了不同深度的UNet网络，并且利用了全尺度的跳跃连接（skip connection）和深度监督（deep supervisions）。另外UNet++还设计一种剪枝方案，加快了UNet++的推理速度。UNet++的结构示意图如下所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/pic_18.png" width="300" height="200"><br><div style="color:#999;font-size:11px;padding:2px">Fig24. UNet++</div></center><p>单纯从结构设计的角度来看，UNet++效果好要归功于其嵌套结构和重新设计的跳跃连接，旨在解决UNet的两个关键挑战:1）优化整体结构的未知深度和2）跳跃连接的不必要的限制性设计。UNet++的一个简单的实现代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchfrom torch <span class="keyword">import</span> nnclass NestedUNet(nn.Module):    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, input_channels=<span class="number">3</span>, deep_supervision=<span class="literal">False</span>, **kwargs</span>):        <span class="built_in">super</span>().__init__()        nb_filter = [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]        self.deep_supervision = deep_supervision        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)        self.up = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)        self.conv0_0 = VGGBlock(input_channels, nb_filter[<span class="number">0</span>], nb_filter[<span class="number">0</span>])        self.conv1_0 = VGGBlock(nb_filter[<span class="number">0</span>], nb_filter[<span class="number">1</span>], nb_filter[<span class="number">1</span>])        self.conv2_0 = VGGBlock(nb_filter[<span class="number">1</span>], nb_filter[<span class="number">2</span>], nb_filter[<span class="number">2</span>])        self.conv3_0 = VGGBlock(nb_filter[<span class="number">2</span>], nb_filter[<span class="number">3</span>], nb_filter[<span class="number">3</span>])        self.conv4_0 = VGGBlock(nb_filter[<span class="number">3</span>], nb_filter[<span class="number">4</span>], nb_filter[<span class="number">4</span>])        self.conv0_1 = VGGBlock(nb_filter[<span class="number">0</span>]+nb_filter[<span class="number">1</span>], nb_filter[<span class="number">0</span>], nb_filter[<span class="number">0</span>])        self.conv1_1 = VGGBlock(nb_filter[<span class="number">1</span>]+nb_filter[<span class="number">2</span>], nb_filter[<span class="number">1</span>], nb_filter[<span class="number">1</span>])        self.conv2_1 = VGGBlock(nb_filter[<span class="number">2</span>]+nb_filter[<span class="number">3</span>], nb_filter[<span class="number">2</span>], nb_filter[<span class="number">2</span>])        self.conv3_1 = VGGBlock(nb_filter[<span class="number">3</span>]+nb_filter[<span class="number">4</span>], nb_filter[<span class="number">3</span>], nb_filter[<span class="number">3</span>])        self.conv0_2 = VGGBlock(nb_filter[<span class="number">0</span>]*<span class="number">2</span>+nb_filter[<span class="number">1</span>], nb_filter[<span class="number">0</span>], nb_filter[<span class="number">0</span>])        self.conv1_2 = VGGBlock(nb_filter[<span class="number">1</span>]*<span class="number">2</span>+nb_filter[<span class="number">2</span>], nb_filter[<span class="number">1</span>], nb_filter[<span class="number">1</span>])        self.conv2_2 = VGGBlock(nb_filter[<span class="number">2</span>]*<span class="number">2</span>+nb_filter[<span class="number">3</span>], nb_filter[<span class="number">2</span>], nb_filter[<span class="number">2</span>])        self.conv0_3 = VGGBlock(nb_filter[<span class="number">0</span>]*<span class="number">3</span>+nb_filter[<span class="number">1</span>], nb_filter[<span class="number">0</span>], nb_filter[<span class="number">0</span>])        self.conv1_3 = VGGBlock(nb_filter[<span class="number">1</span>]*<span class="number">3</span>+nb_filter[<span class="number">2</span>], nb_filter[<span class="number">1</span>], nb_filter[<span class="number">1</span>])        self.conv0_4 = VGGBlock(nb_filter[<span class="number">0</span>]*<span class="number">4</span>+nb_filter[<span class="number">1</span>], nb_filter[<span class="number">0</span>], nb_filter[<span class="number">0</span>])        <span class="keyword">if</span> self.deep_supervision:            self.final1 = nn.Conv2d(nb_filter[<span class="number">0</span>], num_classes, kernel_size=<span class="number">1</span>)            self.final2 = nn.Conv2d(nb_filter[<span class="number">0</span>], num_classes, kernel_size=<span class="number">1</span>)            self.final3 = nn.Conv2d(nb_filter[<span class="number">0</span>], num_classes, kernel_size=<span class="number">1</span>)            self.final4 = nn.Conv2d(nb_filter[<span class="number">0</span>], num_classes, kernel_size=<span class="number">1</span>)        <span class="keyword">else</span>:            self.final = nn.Conv2d(nb_filter[<span class="number">0</span>], num_classes, kernel_size=<span class="number">1</span>)    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):        x0_0 = self.conv0_0(<span class="built_in">input</span>)        x1_0 = self.conv1_0(self.pool(x0_0))        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], <span class="number">1</span>))        x2_0 = self.conv2_0(self.pool(x1_0))        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], <span class="number">1</span>))        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], <span class="number">1</span>))        x3_0 = self.conv3_0(self.pool(x2_0))        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], <span class="number">1</span>))        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], <span class="number">1</span>))        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], <span class="number">1</span>))        x4_0 = self.conv4_0(self.pool(x3_0))        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], <span class="number">1</span>))        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], <span class="number">1</span>))        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], <span class="number">1</span>))        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], <span class="number">1</span>))        <span class="keyword">if</span> self.deep_supervision:            output1 = self.final1(x0_1)            output2 = self.final2(x0_2)            output3 = self.final3(x0_3)            output4 = self.final4(x0_4)            <span class="keyword">return</span> [output1, output2, output3, output4]        <span class="keyword">else</span>:            output = self.final(x0_4)            <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>完整实现过程可参考<a target="_blank" rel="noopener" href="https://github.com/4uiiurz1/pytorch-nested-unet/">GitHub</a>开源代码。</p><p>以上仅对几个主要的语义分割网络模型进行介绍，从当年的FCN到如今的各种模型层出不穷，想要对所有的SOTA模型全部进行介绍已经不太可能。其他诸如ENet、DeconvNet、RefineNet、HRNet、PixelNet、BiSeNet、UpperNet等网络模型，均各有千秋。本小节旨在让大家熟悉语义分割的主要模型结构和设计。深度学习和计算机视觉发展日新月异，一个新的SOTA模型出来，肯定很快就会被更新的结构设计所代替，重点是我们要了解语义分割的发展脉络，对主流的前沿研究能够保持一定的关注。</p><h2 id="5-语义分割训练Tips"><a href="#5-语义分割训练Tips" class="headerlink" title="5. 语义分割训练Tips"></a>5. 语义分割训练Tips</h2><p>PyTorch是一款极为便利的深度学习框架。在日常实验过程中，我们要多积累和总结，假以时日，人人都能总结出一套自己的高效模型搭建和训练套路。这一节我们给出一些惯用的PyTorch代码搭建方式，以及语义分割训练过程中的可视化方法，方便大家在训练过程中能够直观的看到训练效果。</p><h3 id="5-1-PyTorch代码搭建方式"><a href="#5-1-PyTorch代码搭建方式" class="headerlink" title="5.1 PyTorch代码搭建方式"></a>5.1 PyTorch代码搭建方式</h3><p>无论是分类、检测还是分割抑或是其他非视觉的深度学习任务，其代码套路相对来说较为固定，不会跳出基本的代码框架。一个深度学习的实现代码框架无非就是以下五个主要构成部分：</p><ul><li>数据：Data</li><li>模型：Model</li><li>判断：Criterion</li><li>优化：Optimizer</li><li>日志：Logger</li></ul><p>所以一个基本的顺序实现范式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># datadataset = VOC()||COCO()||ADE20K()data_loader = data.DataLoader(dataSet)# modelmodel = ...model_parallel = torch.nn.DataParallel(model)# Criterionloss = criterion(...)# Optimizeroptimer = optim.SGD(...)# Logger and Visulizationvisdom = ...tensorboard = ...textlog = ...# Model Parametersdata_size, batch_size, epoch_size, iterations = ..., ...</span></span><br></pre></td></tr></table></figure><p>不论是哪种深度学习任务，一般都免不了以上五项基本模块。所以一个简单的、相对完整的PyTorch模型项目代码应该是如下结构的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">|-- semantic segmentation example    |-- dataset.py    |-- models        |-- unet.py        |-- deeplabv3.py        |-- pspnet.py        |-- ...    |-- _config.yml    |-- main.py    |-- utils    |   |-- visual.py    |   |-- loss.py    |   |-- ...    |-- README.md    ...</span><br></pre></td></tr></table></figure><p>上面的示例代码结构中，我们把训练和验证相关代码都放到<code>main.py</code>文件中，但在实际实验中，这块的灵活性极大。一般来说，模型训练策略有三种，一种是边训练边验证最后再测试、另一种则是在训练中验证，将验证过程糅合到训练过程中，还有一种最简单，就是训练完了再单独验证和测试。所以，我们这里也可以单独定义对应的函数，训练<code>train()</code>、验证<code>val()</code>以及测试<code>test()</code>除此之外，还有一些辅助功能需要设计，包括打印训练信息<code>print()</code>、绘制损失函数<code>plot()</code>、保存最优模型<code>save()</code>，调整训练参数<code>update()</code>。</p><p>所以训练代码控制流程可以归纳为TVT+PPSU的模式。</p><h3 id="5-2-可视化方法"><a href="#5-2-可视化方法" class="headerlink" title="5.2 可视化方法"></a>5.2 可视化方法</h3><p>PyTorch原生的可视化支持模块是Visdom，当然鉴于TensorFlow的应用广泛性，PyTorch同时也支持TensorBoard的可视化方法。语义分割需要能够直观的看到训练效果，所以在训练过程中辅以一定的可视化方法是十分必要的。</p><h4 id="Visdom"><a href="#Visdom" class="headerlink" title="Visdom"></a>Visdom</h4><p>visdom是一款用于创建、组织和共享实时大量训练数据可视化的灵活工具。深度学习模型训练通常放在远程的服务器上，服务器上训练的一个问题就在于不能方便地对训练进行可视化，相较于TensorFlow的可视化工具TensorBoard，visdom则是对应于PyTorch的可视化工具。直接通过<code>pip install visdom</code>即可完成安装，之后在终端输入如下命令即可启动visdom服务：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m visdom.server </span><br></pre></td></tr></table></figure><p>启动服务后输入本地或者远程地址，端口号8097，即可打开visdom主页。具体到深度学习训练时，我们可以在torch训练代码下插入visdom的可视化模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.steps_plot &gt; <span class="number">0</span> <span class="keyword">and</span> step % args.steps_plot == <span class="number">0</span>:    image = inputs[<span class="number">0</span>].cpu().data    vis.image(image,<span class="string">f&#x27;input (epoch: <span class="subst">&#123;epoch&#125;</span>, step: <span class="subst">&#123;step&#125;</span>)&#x27;</span>)    vis.image(outputs[<span class="number">0</span>].cpu().<span class="built_in">max</span>(<span class="number">0</span>)[<span class="number">1</span>].data, <span class="string">f&#x27;output (epoch: <span class="subst">&#123;epoch&#125;</span>, step: <span class="subst">&#123;step&#125;</span>)&#x27;</span>)    vis.image(targets[<span class="number">0</span>].cpu().data, <span class="string">f&#x27;target (epoch: <span class="subst">&#123;epoch&#125;</span>, step: <span class="subst">&#123;step&#125;</span>)&#x27;</span>)    vis.image(loss, <span class="string">f&#x27;loss (epoch: <span class="subst">&#123;epoch&#125;</span>, step: <span class="subst">&#123;step&#125;</span>)&#x27;</span>)</span><br></pre></td></tr></table></figure><p>visdom效果展示如下：</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/640.jpg" width="400" height="300"><br><div style="color:#999;font-size:11px;padding:2px">Fig25. visdom example</div></center><h4 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h4><p>很多TensorFlow用户更习惯于使用TensorBoard来进行训练的可视化展示。为了能让PyTorch用户也能用上TensorBoard，有开发者提供了PyTorch版本的TensorBoard，也就是tensorboardX。熟悉TensorBoard的用户可以无缝对接到tensorboardX，安装方式为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboardX</span><br></pre></td></tr></table></figure><p>除了要安装PyTorch之外，还需要安装TensorFlow。跟TensorBoard一样，tensorboardX也支持scalar, image, figure, histogram, audio, text, graph, onnx_graph, embedding, pr_curve，video等不同类型对象的可视化展示方式。tensorboardX和TensorBoard的启动方式一样，直接在终端下运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir runs</span><br></pre></td></tr></table></figure><p>一个完整tensorboardX使用demo如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchimport torchvision.utils <span class="keyword">as</span> vutilsimport numpy <span class="keyword">as</span> npimport torchvision.models <span class="keyword">as</span> modelsfrom torchvision <span class="keyword">import</span> datasetsfrom tensorboardX <span class="keyword">import</span> SummaryWriterresnet18 = models.resnet18(<span class="literal">False</span>)writer = SummaryWriter()sample_rate = <span class="number">44</span>100freqs = [<span class="number">262</span>, <span class="number">294</span>, <span class="number">330</span>, <span class="number">349</span>, <span class="number">392</span>, <span class="number">440</span>, <span class="number">440</span>, <span class="number">440</span>, <span class="number">440</span>, <span class="number">440</span>, <span class="number">440</span>]<span class="keyword">for</span> n_iter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):    dummy_s1 = torch.rand(<span class="number">1</span>)    dummy_s2 = torch.rand(<span class="number">1</span>)    <span class="comment"># data grouping by `slash`    writer.add_scalar(&#x27;data/scalar1&#x27;, dummy_s1[0], n_iter)    writer.add_scalar(&#x27;data/scalar2&#x27;, dummy_s2[0], n_iter)    writer.add_scalars(&#x27;data/scalar_group&#x27;, &#123;&#x27;xsinx&#x27;: n_iter * np.sin(n_iter),                                             &#x27;xcosx&#x27;: n_iter * np.cos(n_iter),                                             &#x27;arctanx&#x27;: np.arctan(n_iter)&#125;, n_iter)    dummy_img = torch.rand(32, 3, 64, 64)  # output from network    if n_iter % 10 == 0:        x = vutils.make_grid(dummy_img, normalize=True, scale_each=True)        writer.add_image(&#x27;Image&#x27;, x, n_iter)        dummy_audio = torch.zeros(sample_rate * 2)        for i in range(x.size(0)):            # amplitude of sound should in [-1, 1]            dummy_audio[i] = np.cos(freqs[n_iter // 10] * np.pi * float(i) / float(sample_rate))        writer.add_audio(&#x27;myAudio&#x27;, dummy_audio, n_iter, sample_rate=sample_rate)        writer.add_text(&#x27;Text&#x27;, &#x27;text logged at step:&#x27; + str(n_iter), n_iter)        for name, param in resnet18.named_parameters():            writer.add_histogram(name, param.clone().cpu().data.numpy(), n_iter)        # needs tensorboard 0.4RC or later        writer.add_pr_curve(&#x27;xoxo&#x27;, np.random.randint(2, size=100), np.random.rand(100), n_iter)dataset = datasets.MNIST(&#x27;mnist&#x27;, train=False, download=True)images = dataset.test_data[:100].float()label = dataset.test_labels[:100]features = images.view(100, 784)writer.add_embedding(features, metadata=label, label_img=images.unsqueeze(1))# export scalar data to JSON for external processingwriter.export_scalars_to_json(&quot;./all_scalars.json&quot;)writer.close()</span></span><br></pre></td></tr></table></figure><p>tensorboardX的展示界面如图所示。</p><center><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://github.com/luwill/louwill-python-learning/raw/master/650.jpg" width="500" height="280"><br><div style="color:#999;font-size:11px;padding:2px">Fig26. tensorboardX example</div></center></div><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a target="_blank" rel="noopener" href="https://github.com/mrgloom/awesome-semantic-segmentation">awesome-semantic-segmentation</a></li><li>Long J , Shelhamer E , Darrell T . Fully Convolutional Networks for Semantic Segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 39(4):640-651.</li><li>Ronneberger O , Fischer P , Brox T . U-Net: Convolutional Networks for Biomedical Image Segmentation[J]. 2015.</li><li>Badrinarayanan V , Kendall A , Cipolla R . SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation[J]. 2015.</li><li>Zhao H , Shi J , Qi X , et al. Pyramid Scene Parsing Network[J]. 2016.</li><li>Huang H , Lin L , Tong R , et al. UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation[J]. arXiv, 2020.</li><li>UNet++: A Nested U-Net Architecture for Medical Image Segmentation</li><li><a target="_blank" rel="noopener" href="https://github.com/4uiiurz1/pytorch-nested-unet">https://github.com/4uiiurz1/pytorch-nested-unet</a></li><li>Minaee S , Boykov Y , Porikli F , et al. Image Segmentation Using Deep Learning: A Survey[J]. 2020.</li><li>Guo Y , Liu Y , Georgiou T , et al. A review of semantic segmentation using deep neural networks[J]. International Journal of Multimedia Information Retrieval, 2017.</li><li><a target="_blank" rel="noopener" href="https://github.com/fabioperez/pytorch-examples/">https://github.com/fabioperez/pytorch-examples/</a></li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">机器学习实验室</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://adordly.github.io/2022/02/25/deepLearningSemanticSegmentation/">https://adordly.github.io/2022/02/25/deepLearningSemanticSegmentation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">此文章版权归机器学习实验室所有，如有转载，请註明来自原作者</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></div><div class="post_share"><div class="social-share" data-image="/img/text/deeplearning.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/wechat.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/alipay.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/02/26/%E7%BB%84%E4%BB%B6%E5%8C%96%E5%BC%80%E5%8F%91/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/text/vue.jpeg" onerror='onerror=null,src="/img/404.jpeg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Vue组件开发</div></div></a></div><div class="next-post pull-right"><a href="/2022/02/24/SpringBoot%E6%95%B4%E5%90%88%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E5%BA%93/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/text/Spring1.jpeg" onerror='onerror=null,src="/img/404.jpeg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Springboot整合相关数据库</div></div></a></div></nav><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/social_img.jpeg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">咳咳</div><div class="author-info__description">没有选择会是唯一的路</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div></div><a id="card-info-btn" href="https://adordly.github.io/"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">一名拖更的懒博主</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E6%88%98%E6%8C%87%E5%8D%97"><span class="toc-number">1.</span> <span class="toc-text">深度学习语义分割理论与实战指南</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#A-Theory-and-Practical-Guide-to-Deep-Learning-Semantic-Segmentation"><span class="toc-number">2.</span> <span class="toc-text">A Theory and Practical Guide to Deep Learning Semantic Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">2.1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%A6%82%E8%BF%B0"><span class="toc-number">2.2.</span> <span class="toc-text">1. 语义分割概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E7%BB%84%E4%BB%B6"><span class="toc-number">2.3.</span> <span class="toc-text">2. 关键技术组件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C"><span class="toc-number">2.3.1.</span> <span class="toc-text">2.1 编码器与分类网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%A7%A3%E7%A0%81%E5%99%A8%E4%B8%8E%E4%B8%8A%E9%87%87%E6%A0%B7"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.2 解码器与上采样</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">双线性插值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">转置卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8D%E6%B1%A0%E5%8C%96"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">反池化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Skip-Connection"><span class="toc-number">2.3.3.</span> <span class="toc-text">2.3 Skip Connection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Dilate-Conv%E4%B8%8E%E5%A4%9A%E5%B0%BA%E5%BA%A6"><span class="toc-number">2.3.4.</span> <span class="toc-text">2.4 Dilate Conv与多尺度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E5%90%8E%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF"><span class="toc-number">2.3.5.</span> <span class="toc-text">2.5 后处理技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-%E6%B7%B1%E7%9B%91%E7%9D%A3"><span class="toc-number">2.3.6.</span> <span class="toc-text">2.6 深监督</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-%E9%80%9A%E7%94%A8%E6%8A%80%E6%9C%AF"><span class="toc-number">2.3.7.</span> <span class="toc-text">2.7 通用技术</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.7.1.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B2%BE%E5%BA%A6%E6%8F%8F%E8%BF%B0"><span class="toc-number">2.3.7.2.</span> <span class="toc-text">精度描述</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AEPipeline"><span class="toc-number">2.4.</span> <span class="toc-text">3. 数据Pipeline</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Torch%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E6%A8%A1%E6%9D%BF"><span class="toc-number">2.4.1.</span> <span class="toc-text">3.1 Torch数据读取模板</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-transform%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">2.4.2.</span> <span class="toc-text">3.2 transform与数据增强</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%AE%97%E6%B3%95"><span class="toc-number">2.5.</span> <span class="toc-text">4. 模型与算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-FCN"><span class="toc-number">2.5.1.</span> <span class="toc-text">4.1 FCN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-UNet"><span class="toc-number">2.5.2.</span> <span class="toc-text">4.2 UNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-SegNet"><span class="toc-number">2.5.3.</span> <span class="toc-text">4.3 SegNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Deeplab%E7%B3%BB%E5%88%97"><span class="toc-number">2.5.4.</span> <span class="toc-text">4.4 Deeplab系列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-PSPNet"><span class="toc-number">2.5.5.</span> <span class="toc-text">4.5 PSPNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-UNet"><span class="toc-number">2.5.6.</span> <span class="toc-text">4.6 UNet++</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E8%AE%AD%E7%BB%83Tips"><span class="toc-number">2.6.</span> <span class="toc-text">5. 语义分割训练Tips</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-PyTorch%E4%BB%A3%E7%A0%81%E6%90%AD%E5%BB%BA%E6%96%B9%E5%BC%8F"><span class="toc-number">2.6.1.</span> <span class="toc-text">5.1 PyTorch代码搭建方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%8F%AF%E8%A7%86%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">2.6.2.</span> <span class="toc-text">5.2 可视化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Visdom"><span class="toc-number">2.6.2.1.</span> <span class="toc-text">Visdom</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TensorBoard"><span class="toc-number">2.6.2.2.</span> <span class="toc-text">TensorBoard</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">2.7.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/04/23/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BB%84%E4%BB%B6/" title="分布式组件"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/springcloud/springcloud.jpeg" onerror='this.onerror=null,this.src="/img/404.jpeg"' alt="分布式组件"></a><div class="content"><a class="title" href="/2022/04/23/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BB%84%E4%BB%B6/" title="分布式组件">分布式组件</a><time datetime="2022-04-23T06:23:47.000Z" title="发表于 2022-04-23 14:23:47">2022-04-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/21/git%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" title="git基础知识总结"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/git/git.jpeg" onerror='this.onerror=null,this.src="/img/404.jpeg"' alt="git基础知识总结"></a><div class="content"><a class="title" href="/2022/04/21/git%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" title="git基础知识总结">git基础知识总结</a><time datetime="2022-04-21T06:23:47.000Z" title="发表于 2022-04-21 14:23:47">2022-04-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/19/MySQL%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE(%E4%B9%9D%EF%BC%89%20%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E8%B7%B5/" title="MySQL知识回顾(九):数据库设计与实践"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/mysql/mysql.jpeg" onerror='this.onerror=null,this.src="/img/404.jpeg"' alt="MySQL知识回顾(九):数据库设计与实践"></a><div class="content"><a class="title" href="/2022/04/19/MySQL%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE(%E4%B9%9D%EF%BC%89%20%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E8%B7%B5/" title="MySQL知识回顾(九):数据库设计与实践">MySQL知识回顾(九):数据库设计与实践</a><time datetime="2022-04-19T12:40:47.000Z" title="发表于 2022-04-19 20:40:47">2022-04-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/19/MySQL%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE(%E5%85%AB%EF%BC%89%20DBA%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="MySQL知识回顾(八):DBA常用命令"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/mysql/mysql.jpeg" onerror='this.onerror=null,this.src="/img/404.jpeg"' alt="MySQL知识回顾(八):DBA常用命令"></a><div class="content"><a class="title" href="/2022/04/19/MySQL%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE(%E5%85%AB%EF%BC%89%20DBA%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="MySQL知识回顾(八):DBA常用命令">MySQL知识回顾(八):DBA常用命令</a><time datetime="2022-04-19T12:34:47.000Z" title="发表于 2022-04-19 20:34:47">2022-04-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/19/MySQL%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE(%E4%B8%83%EF%BC%89%20%E8%A7%86%E5%9B%BE/" title="MySQL知识回顾(七):视图"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/mysql/mysql.jpeg" onerror='this.onerror=null,this.src="/img/404.jpeg"' alt="MySQL知识回顾(七):视图"></a><div class="content"><a class="title" href="/2022/04/19/MySQL%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE(%E4%B8%83%EF%BC%89%20%E8%A7%86%E5%9B%BE/" title="MySQL知识回顾(七):视图">MySQL知识回顾(七):视图</a><time datetime="2022-04-19T12:32:47.000Z" title="发表于 2022-04-19 20:32:47">2022-04-19</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(/img/text/deeplearning.png)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 咳咳</div><div class="footer_custom_text">Hi, welcome to my blog</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js").then(()=>{pangu.autoSpacingPage()})}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>function loadValine(){function n(){new Valine(Object.assign({el:"#vcomment",appId:"GsGNLORFmjCB9yFfADJIBAR4-gzGzoHsz",appKey:"DeLHVpVyM3s1NHtFDhAuQeMY",avatar:"monsterid",serverURLs:"",emojiMaps:"",path:window.location.pathname,visitor:!1},null))}"function"==typeof Valine?n():getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js").then(n)}function loadOtherComment(){loadValine()}btf.loadComment(document.getElementById("vcomment"),loadValine)</script></div><script type="text/javascript" src="/js/fairyDustCursor.js"></script><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>